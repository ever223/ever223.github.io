---
layout: post
title: 海量数据操作(排序、Top k、查重等)
date: 2016-07-29 11:40
modified: 2016-07-29 11:40				
category: 面试
tags: ["算法","面试"]
excerpt: 核心思想－分而治之/hash映射 + hash统计 + 堆/快速/归并排序，就是先映射，而后统计，最后排序
comments: true
---






### 海量数据操作

	
**核心思想: 分而治之/hash映射 + hash统计 + 堆/快速/归并排序，就是先映射，而后统计，最后排序**

1. 从100亿数据中找出最大的1000个数？
	1. 建立一个1000个数据的最小堆，依次比较，小于丢弃；大于先出堆，后用大于的数入堆即可。i/o时间和维护堆的时间。
	2. 冒泡，一次读入1亿数据+1000的连续空间，冒泡1000次，在读入下一批数据到前1亿内存里，在冒泡1000次，时间复杂度为1000N+i/o时间。
	3. 计数排序，对范围min,max的数据一次计数存入一个a[min,max]的数组，取出最后1000个数据即可。复杂度o(n)+i/o时间。
2. 对100亿数据进行排序？
	用上述的方法3，计数排序后，依次写回内存。也可以使用归并排序，逐段读入，排序，合并返回；桶排序，把数据按照某种规则映射到几个区间，依次对几个区间排序并返回。
3. 大规模数据查找
	采用B+索引数，红黑树，hash开链方式表
4. 假设一个文件中有9亿条不重复的9位整数，现在要求对这个文件进行排序?
	直接使用java.util.BitSet; 初始值设为10亿；
5. 在某个项目中，我们需要对2亿条手机号码删除重复记录(过滤号码黑名单同样有效) 
	建立一个足够大的bit数组当作hash表	 
	以bit数组的下标来表示一个整数 	
	以bit位中的0或1来表示这个整数是否在这个数组中存在	 
	适用于无重复原始数据的搜索 	
	原来每个整数需要4byte空间变为1bit，空间压缩率为32倍  
	扩展后可实现其他类型（包括重复数据）的搜索	
6. 提取出某日访问百度次数最多的那个IP?
   	1. 分而治之/hash映射：针对数据太大，内存受限，只能是：把大文件化成(取模映射)小文件，即	2. 16字方针：大而化小，各个击破，缩小规模，逐个解决
	3. hash统计：当大文件转化了小文件，那么我们便可以采用常规的Hashmap(ip，value)来进行频率统计。
	4. 堆/快速排序：统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP。   
	
    具体而论，则是： “首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。注意到IP是32位的，最多有个2^32个IP。同样可以采用映射的方法，比如模1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用Hash_map进行频率统计，然后再找出频率最大的几个）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。”
7. 搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。

    　　假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。），请你统计最热门的10个查询串，要求使用的内存不能超过1G。

    　　由上面一题，我们知道，数据大则划为小的，但如果数据规模比较小，能一次性装入内存呢?比如这第2题，虽然有一千万个Query，但是由于重复度比较高，因此事实上只有300万的Query，每个Query255Byte，因此我们可以考虑把他们都放进内存中去，而现在只是需要一个合适的数据结构，在这里，Hash Table绝对是我们优先的选择。所以我们摒弃分而治之/hash映射的方法，直接上hash统计，然后排序。

	1. hash统计：先对这批海量数据预处理(维护一个Key为Query字串，Value为该Query出现次数的HashTable，即Hashmap(Query，Value)，每次读取一个Query，如果该字串不在Table中，那么加入该字串，并且将Value值设为1；如果该字串在Table中，那么将该字串的计数加一即可。最终我们在O(N)的时间复杂度内用Hash表完成了统计；
	2. 堆排序：第二步、借助堆这个数据结构，找出Top K，时间复杂度为N‘logK。即借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比所以，我们最终的时间复杂度是：O（N） + N'\*O（logK），（N为1000万，N’为300万）。	
	
    　　别忘了这篇文章中所述的堆排序思路：“维护k个元素的最小堆，即用容量为k的最小堆存储最先遍历到的k个数，并假设它们即是最大的k个数，建堆费时O（k），并调整堆（费时O（logk））后，有k1>k2>...kmin（kmin设为小顶堆中最小元素）。继续遍历数列，每次遍历一个元素x，与堆顶元素比较，若x>kmin，则更新堆（用时logk），否则不更新堆。这样下来，总费时O（k\*logk+（n-k）\*logk）=O（n\*logk）。此方法得益于在堆中，查找等各项操作时间复杂度均为logk。”    
    当然，你也可以采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。
8. 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？  
	　　可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。  
	　　遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,…,a999）中。这样每个小文件的大约为300M。   
	　　遍历文件b，采取和a相同的方式将url分别存储到1000小文件（记为b0,b1,…,b999）。这样处理后，所有可能相同的url都在对应的小 文件（a0vsb0,a1vsb1,…,a999vsb999）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的 url即可。   
	　　求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。
9. 在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。  
	1. 采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 \* 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。	
	2. 也可采用采用进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。
10. 给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？	
	　　申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。
	> 位图法：   
	> 　　使用位图法判断整形数组是否存在重复
判断集合中存在重复是常见编程任务之一，当集合中数据量比较大时我们通常希望少进行几次扫描，这时双重循环法就不可取了。		
	> 　　位图法比较适合于这种情况，它的做法是按照集合中最大元素max创建一个长度为max+1的新数组，然后再次扫描原数组，遇到几就给新数组的第几位置上 1，如遇到5就给新数组的第六个元素置1，这样下次再遇到5想置位时发现新数组的第六个元素已经是1了，这说明这次的数据肯定和以前的数据存在着重复。这 种给新数组初始化时置零其后置一的做法类似于位图的处理方法故称位图法。它的运算次数最坏的情况为2N。如果已知数组的最大值即能事先给新数组定长的话效 率还能提高一倍。
	
11. 5亿个int找它们的中位数。	
　　这个例子比上面那个更明显。首先我们将int划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。	
　　实际上，如果不是int是int64，我们可以经过3次这样的划分即可降低到可以接受的程度。即可以先将int64分成2^24个区域，然后确定区域的第几大数，在将该区域分成2^20个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有2^20，就可以直接利用direct addr table进行统计了。

12. **Trie树/数据库/倒排索引**

	Trie树

		适用范围：数据量大，重复多，但是数据种类小可以放入内存 
		  
		基本原理及要点：实现方式，节点孩子的表示方式    
		
		扩展：压缩实现。
		
	问题实例： 
	　	  
	1. 有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。 
	　　	    
	2. 1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。请问怎么设计和实现？   
	　　	
	3. 寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。
	　　    
	4. 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词。其解决方法是：用trie树统计每个词出现的次数，时间复杂度是O(n\*le)（le表示单词的平准长度），然后是找出出现最频繁的前10个词。
	　　
　　外排序
　　
	   适用范围：大数据的排序，去重	
	   
	　　基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树 
	　　   
	问题实例：
	
	有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。
	
	这个数据具有很明显的特点，词的大小为16个字节，但是内存只有1m做hash有些不够，所以可以用来排序。内存可以当输入缓冲区使用。